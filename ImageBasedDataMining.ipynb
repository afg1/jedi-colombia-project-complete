{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageBasedDataMining.ipynb\n",
    "‹ ImageBasedDataMining.ipynb › Copyright (C) ‹ 2019 › ‹ Andrew Green - andrew.green-2@manchester.ac.uk › This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "|   Date     |   Who   |   Note                                       |\n",
    "| 20190201   |   afg   |   First version, adapted from MadagaSKA code |\n",
    "| 20190206   |   afg   |   Finished binary IBDM, just needs plotting  |\n",
    "| 20190208   |   afg   |   Update paths to work with docker container |\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to do image based data mining against a binary outcome. The outcome could be whatever you like, provided it is binary; some examples include overall survival, locoregional failure and feeding tube insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries we want and set up the notebook\n",
    "import os\n",
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pandas to load a csv file of the data. Pandas is very powerful - here we will be using only its most basic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clinicalDataPath = \"/data/clinicalData.csv\"\n",
    "\n",
    "clinicalData = pd.read_csv(clinicalDataPath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the clinical data is a column recording what type of treatment a patient had. We can look at that column by indexing the dataframe with the column heading like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinicalData[\"Oncologic Treatment Summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of acronyms going on in this data, the most important are:\n",
    "\n",
    "CMT  = Chemotherapy\n",
    "ERT  = External beam radiotherapy\n",
    "CCRT = Concurrent Chemo-radiotherapy\n",
    "\n",
    "Other information includes what type of treatment was used when in a patient's pathway, for eaxmple:\n",
    "\n",
    "`Surgery --> CMT --> CCRT + Cetuximab`\n",
    "\n",
    "means a patient had surgery, then chemotherapy, then concurrent chemo-radiotherapy with an immunotherapy drug called Cetuximab.\n",
    "\n",
    "The type of treatment a patient recieved is very important, because it affects how they respond. To remove potential confounding effects in our image based data mining, we would like to have all the patients be on as similar a course of treatment as possible. \n",
    "\n",
    "As a first set of patients to try, lets only take patients who started their treatment with CCRT. We will allow patients who had surgery after radiotherapy, because that shouldn't affect their response to the radiation. If we were being really strict, we could also exclude these patients.\n",
    "\n",
    "We can reduce our dataset using some pandas indexing tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccrtOnlyPatients = clinicalData[(clinicalData[\"Oncologic Treatment Summary\"].str.contains('^CCRT', regex=True)) & (clinicalData[\"Oncologic Treatment Summary\"].str.contains('\\+', regex=True) == False)]\n",
    "len(ccrtOnlyPatients[\"Oncologic Treatment Summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain what is going on here:\n",
    "\n",
    "- `[]` indexes the dataframe, getting values out of it. We can do this with a boolean mask \n",
    "- `(clinicalData[\"Oncologic Treatment Summary\"].str.contains('^CCRT', regex=True))` asks \"Does the value in this cell start with CCRT?\" It says True where this is the case\n",
    "- `(clinicalData[\"Oncologic Treatment Summary\"].str.contains('\\+', regex=True) == False)` Asks \"Does the value in this cell not contain a + symbol?\n",
    "- The `&` symbol combines the two logical statements into a boolean mask\n",
    "- We end up with patients who have started their treatment with CCRT, and who didn't get immunotherapy\n",
    "\n",
    "This reduces our original dataset quite a lot, now we only have 60 patients. After we've done some work here, maybe it would be a good idea to relax some of these requirements. You can choose how to relax them by changing the logical statements in this indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our cohort, selected based on their radiotherapy. we need to check a few other things are in order before we continue:\n",
    "\n",
    "- Do all the patients get the same overall dose?\n",
    "- Do all the patients get the same number of fractions?\n",
    "- Do we have all the outcome data we want to be able to do our datamining on these patients?\n",
    "\n",
    "The total dose recieved by the patient is in our database in the column \"RT Total Dose (Gy)\". We can go ahead and histogram this for the patients we have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doseHistogram = ccrtOnlyPatients.hist(column=\"RT Total Dose (Gy)\", bins=20) ## you can adjust many bits of this plot!\n",
    "plt.xlabel(\"RT dose (Gy)\");\n",
    "plt.ylabel(\"# patients\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most patients got a dose of 70 Gy, but there were a few who got higher or lower doses.\n",
    "\n",
    "Let's look at the dose per fraction next by indexing it to get a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccrtOnlyPatients[[\"Dose/Fraction (Gy/fx)\", \"RT Total Dose (Gy)\", \"Number of Fractions\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that all of the patients who got 72 Gy had a non-standard fractionation where they recieved some boost in the last couple of weeks of treatment. These patients always had their treatment in 40 fractions.\n",
    "\n",
    "For simplicity, we will exclude these patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPatients = ccrtOnlyPatients[ccrtOnlyPatients[\"Number of Fractions\"].astype(int) < 40]\n",
    "len(selectedPatients[\"Number of Fractions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice clean dataset where all our patients got nice easy to manipulate fractionation, we need to adjust their doses to take account of the different doses per fraction.\n",
    "\n",
    "We have to do this because of how radiation works: 70 Gy in 33 fractions is a different biologically effective dose than 70 Gy in 35 fractions. If we don't adjust for this, then our results will probably not make any sense.\n",
    "\n",
    "Different fractionations have diferent effects because of the different amount of time normal tissue has to recover. Before we can mine data with different fractionations, we need to standardize somehow. This is usually done with a biologically equivalent dose (BED) calculation, or an equivalent dose @ 2 Gy/fraction (EQD2) calculation. Here we will do a simple BED calculation, whereby we just multiply the real dose by a factor to take account of the differences in fractionation.\n",
    "\n",
    "Also, whether we want to look at 'acute effects' (i.e. things that happen soon) or 'late effects' (i.e. things that happen after a few weeks/months) has a bearing on what we need to do here. The way these differences come into the BED calculation is in an $\\alpha/\\beta$ ratio. A ratio of $\\alpha/\\beta$ = 3.0 is usual when considering late effects.\n",
    "\n",
    "Now we have to choose an endpoint. Let's look at feeding tube insertion, which I think is probably an early effect. That means we need an $\\alpha/\\beta$ of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBEDCorrection(df, early=True):\n",
    "    earlyAlphaBeta = 10.0\n",
    "    lateAlphaBeta  = 03.0\n",
    "#     fractionDoses = df[\"Dose/Fraction (Gy/fx)\"].astype(float)\n",
    "    \n",
    "    if early:\n",
    "        df = df.assign(BEDfactor = lambda d : 1.0 + d[\"Dose/Fraction (Gy/fx)\"].astype(float)/earlyAlphaBeta)\n",
    "    else:\n",
    "        df = df.assign(BEDfactor = lambda d : 1.0 + d[\"Dose/Fraction (Gy/fx)\"].astype(float)/lateAlphaBeta)\n",
    "    return df\n",
    "\n",
    "selectedPatients = calculateBEDCorrection(selectedPatients)\n",
    "\n",
    "selectedPatients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our BED correction, we can start loading data ready to do mining. We will load each image using SimpleITK, convert it to a numpy array and put it into a bit numpy array. We will concurrently load the binary status from the clinical data as well and put that into a numpy array.\n",
    "\n",
    "In the next cell, we will load all our data. Note that we pre-allocate the numpy array we will use to hold the data - this is a performance optimisation to make loading the data a bit quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dosesPath = \"/data/registeredDoses\"\n",
    "\n",
    "\n",
    "probeDose = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(dosesPath, \"{0:04d}.nii\".format(int(2)))))\n",
    "print(probeDose.shape)\n",
    "len(selectedPatients)\n",
    "\n",
    "doseArray = np.zeros((*probeDose.shape, len(selectedPatients)))\n",
    "statusArray = np.zeros((len(selectedPatients),1))\n",
    "\n",
    "print(doseArray.shape)\n",
    "\n",
    "\n",
    "n = 0\n",
    "for idx, pt in selectedPatients.iterrows():\n",
    "    doseArray[...,n] = pt.BEDfactor * sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(dosesPath, \"{0}.nii\".format(pt.ID.split(\"-\")[-1]))))\n",
    "    n += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have the dose and status data loaded, so we can start doing data mining!\n",
    "\n",
    "Here we will defien a function to do binary data mining using a Student t-Test approach. This involves calculating the meand and variance in every voxel for each status group, then using that on a per-voxel basis to caluclate the per-voxel value of t. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagesTTest(doseData, statuses):\n",
    "    \"\"\"\n",
    "    Calculate a per-voxel t statistic between two images. Uses Welford's method to calculate mean and variance. \n",
    "    NB: there is a tricky little bit at the end. Welford's method requires dividing by N to get variance, the T-test \n",
    "    also requires dividing by N, so we just divide by N^2.\n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - statuses: the outcome labels. 1 indicates an event, 0 indicates no event\n",
    "    Returns:\n",
    "        - Tvalues: an array of the same size as one of the images which contains the per-voxel t values\n",
    "    \"\"\"\n",
    "    noEventMean = np.zeros_like(doseData[...,0])\n",
    "    eventMean = np.zeros_like(doseData[...,0])# + np.finfo(float).eps\n",
    "\n",
    "    noEventStd = np.zeros_like(doseData[...,0])\n",
    "    eventStd = np.zeros_like(doseData[...,0])# + np.finfo(float).eps\n",
    "\n",
    "    eventCount = 0\n",
    "    nonEventCount = 0\n",
    "    for n, stat in enumerate(statuses):\n",
    "        if stat == 1 and eventCount == 0:\n",
    "            eventCount += 1.0\n",
    "            eventMean = doseData[...,n]\n",
    "        elif stat == 1:\n",
    "            eventCount += 1.0\n",
    "            om = eventMean.copy()\n",
    "            eventMean = om + (doseData[...,n] - om)/eventCount\n",
    "            eventStd = eventStd + ((doseData[...,n] - om)*(doseData[...,n] - eventMean))\n",
    "\n",
    "\n",
    "        elif stat == 0 and nonEventCount == 0:\n",
    "            nonEventCount += 1.0\n",
    "            noEventMean = doseData[...,n]\n",
    "        elif stat == 0:\n",
    "            nonEventCount += 1.0\n",
    "            om = noEventMean.copy()\n",
    "            noEventMean = om + (doseData[...,n] - om)/nonEventCount\n",
    "            noEventStd = noEventStd + ((doseData[...,n] - om)*(doseData[...,n] - noEventMean))\n",
    "\n",
    "    eventStd /= (eventCount**2)      \n",
    "    noEventStd /= (nonEventCount**2)\n",
    "\n",
    "    Tvalues = np.divide((eventMean - noEventMean), np.sqrt(eventStd + noEventStd))\n",
    "    \n",
    "    return Tvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is pretty big, so let's look at what it's doing!\n",
    "\n",
    "We first create four working arrays, one each for the mean and standard deviation in each group. These arrays are initially filled with zeros.\n",
    "\n",
    "After that, we use a numerically stable method of calculation for the mean and variance of data called Welford's method. The final step converts the variance to standard deviation, then calculates the pixelwise t-value from it and the mean. We then return the t values.\n",
    "\n",
    "Note that the `...` syntax in numpy just means work over all the other axes!\n",
    "\n",
    "\n",
    "\n",
    "We can now use this function with the data we have loaded to get a map of t values, which tells us how the dose affects outcome in different regions of the anatomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tMap = imagesTTest(doseArray, statusArray)\n",
    "\n",
    "referenceAnatomy = sitk.GetArrayFromImage(sitk.ReadImage(\"/data/downsampledCTs/0001.nii\"))\n",
    "tMap = np.random.uniform(size=referenceAnatomy.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "anatomy = plt.imshow(referenceAnatomy[50,...], cmap='Greys_r')\n",
    "tMapOverlay = plt.imshow(tMap[50,...], alpha=0.5)\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have an image of the patient anatomy overlaid with a t-map indicating where dose is having an effect on outcome.\n",
    "\n",
    "This is cool, but it isn't the whole story! We need to assess the statistical significance of the values in the t-map. Because there are so many voxels in the image, we can't do this analytically. This is known as a multiple comparisons problem.\n",
    "\n",
    "The standard way to assess significance in image based data mining is to do a permutation test. In this, we randomly shuffle the labels used to calculate the t-map, and compare the resulting permuted t-map to the original. Anywhere where the 'true' t-map is always more extreme than the permuted t-map is a statistically significant region.\n",
    "\n",
    "Usually, we record a single 'summary statistic' from the permutations. Here we will use the simplest summary statistic possible: the most extreme value.\n",
    "\n",
    "We can write two functions to help us do a permutation test: one to calculate the t-map for a given permutation, and another to organise the information coming from the permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doPermutation(doseData, statuses):\n",
    "    \"\"\"\n",
    "    Permute the statuses and return the maximum t value for this permutation\n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - statuses: the outcome labels. 1 indicates an event, 0 indicates no event. These will be permuted in this function to \n",
    "                    assess the null hypothesis of no dose interaction\n",
    "    Returns:\n",
    "        - (tMin, tMax): the extreme values of the whole t-value map for this permutation\n",
    "    \"\"\"\n",
    "    pstatuses = np.random.permutation(statuses)\n",
    "    permT = imagesTTest(doseData, pstatuses)\n",
    "    return (np.min(permT), np.max(permT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is pretty simple, it uses a numpy function to randomly permute the status labels, then re-calculates the t-map.\n",
    "\n",
    "The returned values from this function are the smallest and largest values. Usually these will be negative and positive respectively, indicating a different effect of the dose on outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutationTest(doseData, statuses, nperm=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to get the global p-value and t-thresholds\n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - statuses: the outcome labels. 1 indicates an event, 0 indicates no event.\n",
    "        - nperm: The number of permutations to calculate. Defaults to 1000 which is the minimum for reasonable accuracy\n",
    "    Returns:\n",
    "        - globalPNeg: the global significance of the test for negative t-values\n",
    "        - globalPPos: the global significance of the test for positive t-values\n",
    "        - tThreshNeg: the list of minT from all the permutations, use it to set a significance threshold.\n",
    "        - tThreshPos: the list of maxT from all the permutations, use it to set a significance threshold.\n",
    "    \"\"\"\n",
    "    tthresh = []\n",
    "    gtCount = 0\n",
    "    ltCount = 0\n",
    "    trueT = imagesTTest(doseData, statuses)\n",
    "    trueMaxT = np.max(trueT)\n",
    "    trueMinT = np.min(trueT)\n",
    "    if haveTQDM:\n",
    "        for perm in tqdm(range(nperm)):\n",
    "            tthresh.append(doPermutation(doseData, statuses))\n",
    "            if tthresh[-1][1] > trueMaxT:\n",
    "                gtCount += 1.0\n",
    "            if tthresh[-1][0] < trueMinT:\n",
    "                ltCount += 1.0\n",
    "    else:\n",
    "        for perm in range(nperm):\n",
    "            tthresh.append(doPermutation(doseData, statuses))\n",
    "            if tthresh[-1][1] > trueMaxT:\n",
    "                gtCount += 1.0\n",
    "            if tthresh[-1][0] < trueMinT:\n",
    "                ltCount += 1.0\n",
    "    \n",
    "    globalpPos = gtCount / float(nperm)\n",
    "    globalpNeg = ltCount / float(nperm)\n",
    "    tthresh = np.array(tthresh)\n",
    "    return (globalpNeg, globalpPos, sorted(tthresh[:,0]), sorted(tthresh[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function organises the data returned from the doPermutation function, and returns us some global summary statistics, as well as an ordered array of maximum and minimum t values from the permutations, that can be used to place significance thresholds on the t-map from the true status labels.\n",
    "\n",
    "The nperm argument to this function is important. It is the number of permutations we will perform - this impacts the minimum p-value we can see (1/nperm) but more importantly impacts the length of time the analysis takes to run. A standard number of permutations would be 1000, though this might take a while!\n",
    "\n",
    "Let's apply the permutation test to our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pNeg, pPos, threshNeg, threshPos = premutationTest(doseArray, statusArray, nperm=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our map of T values, and the associated permutation test distribution, we can plot the regions of significance overlaid on the t-map and CT anatomy. To do this, we use matplotlib's imshow and contourf functions as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = plt.add_subplot(111)\n",
    "\n",
    "# First show the CT\n",
    "ctImg = ax.imshow(referenceCT, cmap='Greys_r')\n",
    "\n",
    "# Now add the t-map with some transparency\n",
    "tmapImg = ax.imshow(tMap, alpha=0.3)\n",
    "\n",
    "## Now do the contourplot at the 95% level for p=0.05\n",
    "contourplot = plt.contourf(tMap, levels=[p005])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
